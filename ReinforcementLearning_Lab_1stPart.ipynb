{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSa0uCaPzh97ilMdHOsSat",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshaghjahangiri/RL-Lab-AMASES-Summer-School/blob/main/ReinforcementLearning_Lab_1stPart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement Learning: Solving Frozen Lake**\n",
        "\n",
        "In this file, we will solve some interesting problems using Reinforcement Learning (RL) algorithms: **SARSA** and **Q-Learning**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Let's Play a Game!**\n",
        "\n",
        "The name of the game is **FROZEN LAKE!** We want to create an agent who can successfully complete the game!\n",
        "\n",
        "### **What is the game? What are the rules?**\n",
        "\n",
        "---\n",
        "\n",
        "### **Objective:**\n",
        "\n",
        "The agent's goal is to navigate from the starting point (**S**) to the goal (**G**) on a grid of tiles.\n",
        "\n",
        "---\n",
        "\n",
        "### **Grid:**\n",
        "\n",
        "Typically, the grid can be **4x4** or **8x8**, containing:\n",
        "\n",
        "- **S**: Starting point (always safe)\n",
        "- **F**: Frozen surface (safe)\n",
        "- **H**: Hole (causes the agent to fall and lose)\n",
        "- **G**: Goal (where the agent needs to reach)\n",
        "\n",
        "---\n",
        "\n",
        "### **Actions:**\n",
        "\n",
        "The agent can move in four directions:\n",
        "\n",
        "- **0**: Left\n",
        "- **1**: Down\n",
        "- **2**: Right\n",
        "- **3**: Up\n",
        "\n",
        "---\n",
        "\n",
        "### **Dynamics:**\n",
        "\n",
        "- If `is_slippery=True`, the agent's actions might not always result in the intended direction (stochastic transitions).\n",
        "- If `is_slippery=False`, the agent's actions result in deterministic transitions.\n",
        "\n",
        "---\n",
        "\n",
        "Let's get started and see if we can train an agent to master the **Frozen Lake** game using RL algorithms!\n"
      ],
      "metadata": {
        "id": "Yip5QAOz3YRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "bZM5ruilbiMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!apt-get install -y xvfb\n",
        "!pip install pyvirtualdisplay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KIJP7vIjdKbF",
        "outputId": "7aa5a964-314b-454f-fd20-b357e07fbebd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 7,813 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.11 [28.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.11 [863 kB]\n",
            "Fetched 7,813 kB in 2s (4,556 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 123576 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.11_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.11_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WLTmtF8D3Swm"
      },
      "outputs": [],
      "source": [
        "# pandas: A powerful data manipulation and analysis library for Python,\n",
        "#providing data structures like DataFrame for managing and analyzing data efficiently.\n",
        "import pandas as pd\n",
        "\n",
        "# numpy: A fundamental library for scientific computing in Python,\n",
        "#offering support for arrays, matrices, and a wide range of mathematical functions.\n",
        "import numpy as np\n",
        "\n",
        "# gymnasium: A toolkit for developing and comparing reinforcement learning algorithms,\n",
        "# featuring a wide variety of environments.\n",
        "import gymnasium as gym\n",
        "\n",
        "# pygame: A cross-platform set of Python modules designed for writing video games,\n",
        "#providing functionalities for graphics, sound, and input handling.\n",
        "import pygame\n",
        "\n",
        "# time: A module that provides various time-related functions,\n",
        "#such as measuring time intervals and delays.\n",
        "import time\n",
        "\n",
        "# os: A module that provides a way of using operating system-dependent\n",
        "#functionality like reading or writing to the file system.\n",
        "import os\n",
        "\n",
        "# matplotlib.pyplot: A plotting library for the Python programming language\n",
        "#and its numerical mathematics extension NumPy, offering a MATLAB-like interface.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# IPython.display: A submodule of IPython for rich display and visualization\n",
        "#in Jupyter Notebooks, allowing the embedding of images, videos, HTML, and more.\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# pyvirtualdisplay: A Python wrapper for Xvfb, a virtual framebuffer, which allows\n",
        "#headless display for running applications that require a graphical user interface.\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# pickle: A module for serializing and de-serializing Python object structures,\n",
        "# allowing for the saving and loading of Python objects.\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def render_frozen_lake():\n",
        "    env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True, render_mode='rgb_array')\n",
        "    env.reset()\n",
        "    img = env.render()\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')  # Turn off axis\n",
        "    plt.title(\"Frozen Lake Environment\")\n",
        "    plt.show()\n",
        "    env.close()\n",
        "\n",
        "render_frozen_lake()"
      ],
      "metadata": {
        "id": "kBPyCvCqxEnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment\n",
        "\n",
        "Using Open AI gym to define the environment of the game\n",
        "\n",
        "Link: https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "\n"
      ],
      "metadata": {
        "id": "VJvDCQijdfvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='rgb_array') # Using FrozenLake-v1"
      ],
      "metadata": {
        "id": "C_byCoXadfSe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of observation states:- \", env.observation_space.n)\n",
        "print(\"Number of action space :- \", env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD7ifDoHemyO",
        "outputId": "ed18bde3-fa9f-4d76-8720-5dfda35dd736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of observation states:-  16\n",
            "Number of action space :-  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code env.reset() in a reinforcement learning environment is used to reset the environment to an initial state\n",
        "# state 0\n",
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2JLkfer8_tS",
        "outputId": "70f2e34d-a40d-4c94-fff4-d9af452960f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, {'prob': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code env.observation_space returns the space that represents\n",
        "# all possible observations that the environment can return.\n",
        "env.observation_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWbSH3Sf8_xe",
        "outputId": "df6aae62-0413-445c-c9d9-c746cec6b174"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(16)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code env.action_space returns the space that represents all possible actions\n",
        "# that can be taken in the environment.\n",
        "env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq4xNpIEDyea",
        "outputId": "08b3bb79-454c-4160-f06c-15cc398e7d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(4)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code random_action = env.action_space.sample() generates a random action\n",
        "# that is valid within the environment's action space.\n",
        "random_action = env.action_space.sample()\n",
        "random_action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6G0Uft4D9so",
        "outputId": "9acc5e1f-927f-4df6-f44e-40efb512c37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code return_value = env.step(random_action) takes the random action generated\n",
        "# and applies it to the environment.\n",
        "# it returns Observation (after taking action), Reward, Truncation, Done, Info\n",
        "return_value = env.step(random_action)\n",
        "return_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALzBisOYEKOh",
        "outputId": "5881af83-65a0-4479-85f3-7ba0326cc5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0.0, False, False, {'prob': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code env.unwrapped.P[9][2] accesses the transition dynamics of the unwrapped environment. Specifically, it retrieves\n",
        "# information about what happens when action 2 is taken from state 9.\n",
        "# Probbility of this transition, Next state, Reward, Done\n",
        "env.unwrapped.P[9][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw4DnxnhGcQ6",
        "outputId": "23ba0c05-cdb7-49b2-eb6d-b1cbcf06943a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1.0, 10, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's see how a boy walks through the environment"
      ],
      "metadata": {
        "id": "yRNvsHClyACi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sets up and starts a virtual display.\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVDhDcqsJ_Hn",
        "outputId": "6fbbcaa3-5918-490e-f3a2-4899b086c3e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7a4866c66c20>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 3 # number of episodes\n",
        "# define the environment\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='rgb_array') # Using FrozenLake-v1\n",
        "# initialize the Q-table\n",
        "Qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# rewards array to save rewards\n",
        "rewards_episode = np.zeros(episodes)\n",
        "\n",
        "for i in range(episodes):\n",
        "\n",
        "    state = env.reset()[0] # reset the environment so the agent is at state 0\n",
        "    terminated = False # True when it reaches the goal or fall in hole\n",
        "    truncated = False # True when actions > 200\n",
        "\n",
        "    while(not terminated and not truncated):\n",
        "\n",
        "        action = env.action_space.sample() # taking a random action in the environment\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action) # response of the environment to the action taken by the agent\n",
        "        state = next_state\n",
        "\n",
        "        # rendering environment to see the walk by the agent\n",
        "\n",
        "        screen = env.render()\n",
        "        plt.imshow(screen)\n",
        "        ipythondisplay.clear_output(wait=True)\n",
        "        ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    if reward == 1:\n",
        "        rewards_episode[i] = 1\n",
        "\n",
        "\n",
        "    #\n",
        "plt.clf()\n",
        "env.close()\n",
        "\n",
        "print(np.sum(rewards_episode))\n",
        "\n"
      ],
      "metadata": {
        "id": "-sDE8V0QzBlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Teaching the Agent: SARSA Algorithm**\n",
        "\n",
        "You saw that just by taking random actions, the possibility of reaching the goal is very close to zero! **Let's teach the agent how it should learn by itself.**\n",
        "\n",
        "---\n",
        "\n",
        "## **SARSA Algorithm**\n",
        "\n",
        "**SARSA** stands for State-Action-Reward-State-Action. It is a type of temporal difference learning method.\n",
        "\n",
        "### **Key Characteristics:**\n",
        "\n",
        "- **Model-Free**: No need to model the environment.\n",
        "- **On-Policy**: Learns the value of the policy being followed.\n",
        "\n",
        "### **Algorithm Steps:**\n",
        "\n",
        "1. **Choose** learning rate (α) and discount factor (γ).\n",
        "2. **Initialize** Q(s, a) for all state-action pairs.\n",
        "3. **Initialize** state (S).\n",
        "4. **Choose** action A from all available actions in state S, A(S), using an epsilon-greedy policy.\n",
        "\n",
        "5. **Loop** until convergence or for a specified number of iterations:\n",
        "   1. Take action A, get reward R, and reach state S' (next state).\n",
        "   2. Choose action A' (next action) from all available actions in state S', A'(S'), using an epsilon-greedy policy.\n",
        "   3. Update Q-value: `Q(s, a) ← Q(s, a) + α [R + γ * Q(s', a') - Q(s, a)]`\n",
        "   4. Update state and action: `S ← S'`, `A ← A'`\n",
        "\n",
        "---\n",
        "\n",
        "With the SARSA algorithm, we aim to train our agent to make better decisions by learning from its experiences. Let's dive into the implementation and see how it works in practice!\n"
      ],
      "metadata": {
        "id": "FJqeAZYxPQ9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining function for epsilon-greedy policy which take a random action when a random number is less than epsilon,\n",
        "# and when it is greater than epsilon take the action which has the highest q-value in the current state\n",
        "\n",
        "def epsilon_greedy_policy(qtable, state, epsilon, env):\n",
        "    # write your code here:\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "id": "qgXYzNzIQFhP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining function for updating q-value for SARSA method\n",
        "\n",
        "def update_Qtable_SARSA(qtable, state, action, reward, next_state, next_action, alpha, gamma):\n",
        "    # write your code down here:\n",
        "\n",
        "    return qtable"
      ],
      "metadata": {
        "id": "Vc_FGxcjUq8Z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create a function to run the agent while learning how to reach the goal in the environment"
      ],
      "metadata": {
        "id": "Qo_gz1A9VNvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_sarsa(episodes, render=False, decay=False):\n",
        "\n",
        "    env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
        "\n",
        "    Qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    alpha = 0.9 # learning rate\n",
        "    gamma = 0.9 # gamma\n",
        "\n",
        "    epsilon = 0.8 # 1 means always the agent takes a random action\n",
        "\n",
        "    epsilon_decay_rate = 0.0001\n",
        "\n",
        "    rng = np.random.default_rng() # generate random number\n",
        "\n",
        "    rewards_episode = np.zeros(episodes)\n",
        "\n",
        "    for i in range(episodes):\n",
        "\n",
        "        state = env.reset()[0] # reset the environment so the agent is at state 0\n",
        "        terminated = False # True when it reaches the goal or fall in hole\n",
        "        truncated = False # True when actions > 200\n",
        "\n",
        "        action = epsilon_greedy_policy(Qtable, state, epsilon, env)\n",
        "\n",
        "        while(not terminated and not truncated):\n",
        "\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            next_action = epsilon_greedy_policy(Qtable, next_state, epsilon, env)\n",
        "\n",
        "            Qtable = update_Qtable_SARSA(Qtable, state, action, reward, next_state, next_action, alpha, gamma) # Update Qtable\n",
        "\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "            if render:\n",
        "              screen = env.render()\n",
        "\n",
        "              plt.imshow(screen)\n",
        "              ipythondisplay.clear_output(wait=True)\n",
        "              ipythondisplay.display(plt.gcf())\n",
        "              time.sleep(1)\n",
        "\n",
        "\n",
        "      # count a successful episode to analyze the performance of the agent\n",
        "        plt.clf()\n",
        "\n",
        "        if decay == True:\n",
        "              epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "              if (epsilon==0):\n",
        "                    alpha = 0.0001\n",
        "\n",
        "        if reward == 1:\n",
        "              rewards_episode[i] = 1\n",
        "\n",
        "\n",
        "    print(\"The number of successful episodes are: \", np.sum(rewards_episode))\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    sum_rewards = np.zeros(episodes)\n",
        "\n",
        "    for e in range(episodes):\n",
        "      sum_rewards[e] = np.sum(rewards_episode[max(0, e - 100): (e+1)])\n",
        "    plt.plot(sum_rewards)\n",
        "    plt.savefig('frozen_lake_sarsa.png')\n",
        "\n",
        "    print('Q-table:\\n', Qtable)\n",
        "    f = open('qtable_updated_sarsa.pkl', 'wb')\n",
        "    pickle.dump(Qtable, f)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "uL_mGuZ-yS9K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_sarsa(10000, decay=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8wSaKwYNteTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, let's use the updated Qtable for running another 100 episodes instead of initializing it with **zeros**!"
      ],
      "metadata": {
        "id": "Jfnqj00S03mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_sarsa_updatedQ(episodes, render=False):\n",
        "\n",
        "    env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
        "\n",
        "    f = open('qtable_updated_sarsa.pkl', 'rb')\n",
        "    Qtable = pickle.load(f)\n",
        "    f.close()\n",
        "    print('Trained Q-table is:\\n ', Qtable)\n",
        "\n",
        "    rewards_episode = np.zeros(episodes)\n",
        "\n",
        "    for i in range(episodes):\n",
        "\n",
        "        state = env.reset()[0] # reset the environment so the agent is at state 0\n",
        "        terminated = False # True when it reaches the goal or fall in hole\n",
        "        truncated = False # True when actions > 200\n",
        "\n",
        "\n",
        "        while(not terminated and not truncated):\n",
        "\n",
        "            action = np.argmax(Qtable[state, :])\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if render:\n",
        "              screen = env.render()\n",
        "              plt.imshow(screen)\n",
        "              ipythondisplay.clear_output(wait=True)\n",
        "              ipythondisplay.display(plt.gcf())\n",
        "\n",
        "              time.sleep(1)\n",
        "\n",
        "\n",
        "      # count a successful episode to analyze the performance of the agent\n",
        "        plt.clf()\n",
        "        if reward == 1:\n",
        "              rewards_episode[i] = 1\n",
        "\n",
        "\n",
        "    print(\"The number of successful episodes are: \", np.sum(rewards_episode))\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    sum_rewards = np.zeros(episodes)\n",
        "\n",
        "    for e in range(episodes):\n",
        "      sum_rewards[e] = np.sum(rewards_episode[max(0, e - 100): (e+1)])\n",
        "\n",
        "    plt.plot(sum_rewards)\n",
        "    plt.savefig('frozen_lake_sarsa.png')"
      ],
      "metadata": {
        "id": "rJrxS0iJwD2c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_sarsa_updatedQ(1, render=True)"
      ],
      "metadata": {
        "id": "NmGYb83_2j0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete SARSA method"
      ],
      "metadata": {
        "id": "vdm5sc1WGuP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_SARSA(episodes, render=False, decay=False, training=True, slippery=False):\n",
        "\n",
        "    env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=slippery, render_mode='rgb_array')\n",
        "\n",
        "    if training:\n",
        "      Qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    else:\n",
        "      f = open('qtable_updated_sarsa.pkl', 'rb')\n",
        "      Qtable = pickle.load(f)\n",
        "      f.close()\n",
        "\n",
        "    alpha = 0.9 # learning rate\n",
        "    gamma = 0.9 # gamma, discount rate\n",
        "\n",
        "    epsilon = 1 # 1 means always the agent takes a random action\n",
        "\n",
        "    epsilon_decay_rate = 0.0001\n",
        "\n",
        "    rng = np.random.default_rng() # generate random number\n",
        "\n",
        "    rewards_episode = np.zeros(episodes)\n",
        "\n",
        "    for i in range(episodes):\n",
        "\n",
        "        state = env.reset()[0] # reset the environment so the agent is at state 0\n",
        "        terminated = False # True when it reaches the goal or fall in hole\n",
        "        truncated = False # True when actions > 200\n",
        "\n",
        "        if training:\n",
        "            action = epsilon_greedy_policy(Qtable, state, epsilon, env)\n",
        "\n",
        "        while(not terminated and not truncated):\n",
        "\n",
        "            if training:\n",
        "\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "                next_action = epsilon_greedy_policy(Qtable, next_state, epsilon, env)\n",
        "\n",
        "                Qtable = update_Qtable_SARSA(Qtable, state, action, reward, next_state, next_action, alpha, gamma) # Update Qtable\n",
        "\n",
        "                state = next_state\n",
        "                action = next_action\n",
        "\n",
        "            else:\n",
        "                action = np.argmax(Qtable[state, :])\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                state = next_state\n",
        "\n",
        "\n",
        "            if render:\n",
        "              screen = env.render()\n",
        "\n",
        "              plt.imshow(screen)\n",
        "              ipythondisplay.clear_output(wait=True)\n",
        "              ipythondisplay.display(plt.gcf())\n",
        "\n",
        "\n",
        "      # count a successful episode to analyze the performance of the agent\n",
        "        plt.clf()\n",
        "\n",
        "        if decay == True:\n",
        "            epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "            if (epsilon==0):\n",
        "                  alpha = 0.0001\n",
        "        if reward == 1:\n",
        "              rewards_episode[i] = 1\n",
        "\n",
        "\n",
        "    print(\"The number of successful episodes are: \", np.sum(rewards_episode))\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    sum_rewards = np.zeros(episodes)\n",
        "\n",
        "    for e in range(episodes):\n",
        "      sum_rewards[e] = np.sum(rewards_episode[max(0, e - 100): (e+1)])\n",
        "    plt.plot(sum_rewards)\n",
        "    plt.savefig('frozen_lake_sarsa.png')\n",
        "\n",
        "    if training:\n",
        "\n",
        "        f = open('qtable_updated_sarsa.pkl', 'wb')\n",
        "        pickle.dump(Qtable, f)\n",
        "        f.close()"
      ],
      "metadata": {
        "id": "t36nQr6iGtwJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_SARSA(15000, training=True, slippery=True)"
      ],
      "metadata": {
        "id": "hXtJ5FXcKFd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_SARSA(15000, training=False, slippery=True)"
      ],
      "metadata": {
        "id": "sdb_5UAJKYEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Teaching the Agent: Q-Learning Algorithm**\n",
        "\n",
        "**Now**, let's teach the agent how it can learn using Q-Learning:\n",
        "\n",
        "---\n",
        "\n",
        "## **Q-Learning**\n",
        "\n",
        "**Q-Learning** is a type of temporal difference learning method.\n",
        "\n",
        "### **Key Characteristics:**\n",
        "\n",
        "- **Model-Free**: No need to model the environment.\n",
        "- **Off-Policy**: Learns the value of the optimal policy independently of the agent's actions.\n",
        "\n",
        "### **Algorithm Steps:**\n",
        "\n",
        "1. **Choose** learning rate (α) and discount factor (γ).\n",
        "2. **Initialize** Q-table (Q-value function) for all state-action pairs.\n",
        "3. **Initialize** state (s).\n",
        "\n",
        "4. **Loop** until convergence or for a specified number of iterations:\n",
        "   1. Take action a, get reward R, and reach state s' (next state).\n",
        "   2. Update Q-value: `Q(s, a) ← Q(s, a) + α [R + γ * max_a Q(s', a) - Q(s, a)]`\n",
        "   3. Update state: `s ← s'`\n",
        "\n",
        "---\n",
        "\n",
        "With the Q-Learning algorithm, we aim to train our agent to make optimal decisions by learning from its experiences. Let's proceed to the implementation and observe the results!\n"
      ],
      "metadata": {
        "id": "B0q2JnHJPcxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining function for updating q-value for Q-Learning method\n",
        "\n",
        "\n",
        "def update_Qtable_Qlearning(qtable, state, action, reward, next_state, alpha, gamma):\n",
        "    # write your code down here:\n",
        "\n",
        "    return qtable"
      ],
      "metadata": {
        "id": "iN2FrXVt_WiO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ql(episodes, render=False, decay=False):\n",
        "\n",
        "    env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
        "\n",
        "    Qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    alpha = 0.9 # learning rate\n",
        "    gamma = 0.9 # gamma\n",
        "\n",
        "    epsilon = 0.8 # 1 means always the agent takes a random action\n",
        "\n",
        "    epsilon_decay_rate = 0.0001\n",
        "\n",
        "    rng = np.random.default_rng() # generate random number\n",
        "\n",
        "    rewards_episode = np.zeros(episodes)\n",
        "\n",
        "    for i in range(episodes):\n",
        "\n",
        "        state = env.reset()[0] # reset the environment so the agent is at state 0\n",
        "        terminated = False # True when it reaches the goal or fall in hole\n",
        "        truncated = False # True when actions > 200\n",
        "\n",
        "        while(not terminated and not truncated):\n",
        "\n",
        "            action = epsilon_greedy_policy(Qtable, state, epsilon, env)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "\n",
        "\n",
        "            Qtable = update_Qtable_Qlearning(Qtable, state, action, reward, next_state, alpha, gamma) # Update Qtable\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if render:\n",
        "              screen = env.render()\n",
        "\n",
        "              plt.imshow(screen)\n",
        "              ipythondisplay.clear_output(wait=True)\n",
        "              ipythondisplay.display(plt.gcf())\n",
        "\n",
        "\n",
        "      # count a successful episode to analyze the performance of the agent\n",
        "        plt.clf()\n",
        "        if decay == True:\n",
        "            epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "            if (epsilon==0):\n",
        "                alpha = 0.0001\n",
        "        if reward == 1:\n",
        "              rewards_episode[i] = 1\n",
        "\n",
        "\n",
        "    print(\"The number of successful episodes are: \", np.sum(rewards_episode))\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    sum_rewards = np.zeros(episodes)\n",
        "\n",
        "    for e in range(episodes):\n",
        "      sum_rewards[e] = np.sum(rewards_episode[max(0, e - 100): (e+1)])\n",
        "    plt.plot(sum_rewards)\n",
        "    plt.savefig('frozen_lake_sarsa.png')\n",
        "\n",
        "    f = open('qtable_updated_ql.pkl', 'wb')\n",
        "    pickle.dump(Qtable, f)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "hADvLqD1_sCa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_ql(15000, decay=True)"
      ],
      "metadata": {
        "id": "BFzXhQY-_2N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare with the SARSA method, Q-Learning method has around 1000 more rewards!!!!!"
      ],
      "metadata": {
        "id": "3R5MYpHBCvwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ql_updatedQtable(episodes, render=False):\n",
        "\n",
        "    env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
        "\n",
        "    f = open('qtable_updated_ql.pkl', 'rb')\n",
        "    Qtable = pickle.load(f)\n",
        "    f.close()\n",
        "\n",
        "    rewards_episode = np.zeros(episodes)\n",
        "\n",
        "    for i in range(episodes):\n",
        "\n",
        "        state = env.reset()[0] # reset the environment so the agent is at state 0\n",
        "        terminated = False # True when it reaches the goal or fall in hole\n",
        "        truncated = False # True when actions > 200\n",
        "\n",
        "        while(not terminated and not truncated):\n",
        "\n",
        "            action = np.argmax(Qtable[state, :])\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if render:\n",
        "              screen = env.render()\n",
        "\n",
        "              plt.imshow(screen)\n",
        "              ipythondisplay.clear_output(wait=True)\n",
        "              ipythondisplay.display(plt.gcf())\n",
        "\n",
        "              time.sleep(1)\n",
        "\n",
        "      # count a successful episode to analyze the performance of the agent\n",
        "        plt.clf()\n",
        "\n",
        "        if reward == 1:\n",
        "              rewards_episode[i] = 1\n",
        "\n",
        "\n",
        "    print(\"The number of successful episodes are: \", np.sum(rewards_episode))\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    sum_rewards = np.zeros(episodes)\n",
        "\n",
        "    for e in range(episodes):\n",
        "      sum_rewards[e] = np.sum(rewards_episode[max(0, e - 100): (e+1)])\n",
        "    plt.plot(sum_rewards)\n",
        "    plt.savefig('frozen_lake_ql_updated.png')"
      ],
      "metadata": {
        "id": "YG7bTVSBAfbh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_ql_updatedQtable(1, render=True)"
      ],
      "metadata": {
        "id": "rrktHxrN_2Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Q-Learning method"
      ],
      "metadata": {
        "id": "IoOvy9VaKzI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_QL(episodes, render=False, decay=False, training=True, slippery=False):\n",
        "\n",
        "    env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=slippery, render_mode='rgb_array')\n",
        "\n",
        "    if training:\n",
        "      Qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    else:\n",
        "      f = open('qtable_updated_ql.pkl', 'rb')\n",
        "      Qtable = pickle.load(f)\n",
        "      f.close()\n",
        "\n",
        "    alpha = 0.9 # learning rate\n",
        "    gamma = 0.9 # gamma, discount rate\n",
        "\n",
        "    epsilon = 1 # 1 means always the agent takes a random action\n",
        "\n",
        "    epsilon_decay_rate = 0.0001\n",
        "\n",
        "    rng = np.random.default_rng() # generate random number\n",
        "\n",
        "    rewards_episode = np.zeros(episodes)\n",
        "\n",
        "    for i in range(episodes):\n",
        "\n",
        "        state = env.reset()[0] # reset the environment so the agent is at state 0\n",
        "        terminated = False # True when it reaches the goal or fall in hole\n",
        "        truncated = False # True when actions > 200\n",
        "\n",
        "\n",
        "        while(not terminated and not truncated):\n",
        "\n",
        "            if training:\n",
        "\n",
        "                action = epsilon_greedy_policy(Qtable, state, epsilon, env)\n",
        "\n",
        "            else:\n",
        "                action = np.argmax(Qtable[state, :])\n",
        "\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            if training:\n",
        "\n",
        "                Qtable = update_Qtable_Qlearning(Qtable, state, action, reward, next_state, alpha, gamma) # Update Qtable\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "\n",
        "            if render:\n",
        "              screen = env.render()\n",
        "\n",
        "              plt.imshow(screen)\n",
        "              ipythondisplay.clear_output(wait=True)\n",
        "              ipythondisplay.display(plt.gcf())\n",
        "\n",
        "              time.sleep(1)\n",
        "\n",
        "\n",
        "      # count a successful episode to analyze the performance of the agent\n",
        "        plt.clf()\n",
        "\n",
        "        if decay == True:\n",
        "            epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "            if (epsilon==0):\n",
        "                  alpha = 0.0001\n",
        "        if reward == 1:\n",
        "              rewards_episode[i] = 1\n",
        "\n",
        "\n",
        "    print(\"The number of successful episodes are: \", np.sum(rewards_episode))\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    sum_rewards = np.zeros(episodes)\n",
        "\n",
        "    for e in range(episodes):\n",
        "      sum_rewards[e] = np.sum(rewards_episode[max(0, e - 100): (e+1)])\n",
        "\n",
        "    if not render:\n",
        "        plt.plot(sum_rewards)\n",
        "        plt.savefig('frozen_lake_ql.png')\n",
        "\n",
        "    if training:\n",
        "\n",
        "        f = open('qtable_updated_ql.pkl', 'wb')\n",
        "        pickle.dump(Qtable, f)\n",
        "        f.close()"
      ],
      "metadata": {
        "id": "IHqsyK57K2Ry"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_QL(1, decay=True, training=False, slippery=True, render=True)"
      ],
      "metadata": {
        "id": "ClKnEjQfMNY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Greedy-GQ algorithm"
      ],
      "metadata": {
        "id": "JvtiDv4dENjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for updating the weight vector using the Greedy-GQ algorithm\n",
        "def update_weights_GreedyGQ(w, state, action, reward, next_state, next_action, alpha, beta, gamma, phi_s_a, phi_s_next_a):\n",
        "    \"\"\"\n",
        "    Update the weights using the Greedy-GQ algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    w: Current parameter vector\n",
        "    state: Current state\n",
        "    action: Current action\n",
        "    reward: Reward received after taking the action\n",
        "    next_state: Next state after taking the action\n",
        "    next_action: Next action taken in the next state\n",
        "    alpha: Learning rate for Q-values\n",
        "    beta: Learning rate for parameter vector w\n",
        "    gamma: Discount factor\n",
        "    phi_s_a: Feature vector for current state-action pair\n",
        "    phi_s_next_a: Feature vector for next state-action pair\n",
        "\n",
        "    Returns:\n",
        "    w: Updated parameter vector\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the temporal difference error (TD error)\n",
        "    td_error = reward + gamma * np.dot(w, phi_s_next_a) - np.dot(w, phi_s_a)\n",
        "\n",
        "    # Update the parameter vector w\n",
        "    w += beta * (td_error * phi_s_a - gamma * (np.dot(phi_s_a, phi_s_next_a)) * phi_s_next_a)\n",
        "\n",
        "    # Update the Q-values using the TD error\n",
        "    w += alpha * td_error * phi_s_a\n",
        "\n",
        "    return w\n"
      ],
      "metadata": {
        "id": "mr1w33nxEMnn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to select action using epsilon-greedy policy\n",
        "def epsilon_greedy_policy_gq(state, w, epsilon, n_actions, n_states):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(n_actions)\n",
        "    else:\n",
        "        q_values = np.array([np.dot(w, get_feature_vector(state, action, n_actions, n_states)) for action in range(n_actions)])\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "# Function to get the feature vector for a state-action pair\n",
        "def get_feature_vector(state, action, n_actions, n_states):\n",
        "    # Combine state and action into a single feature vector\n",
        "    phi = np.zeros(n_actions * n_states)\n",
        "    phi[state * n_actions + action] = 1\n",
        "    return phi"
      ],
      "metadata": {
        "id": "KikHq8DpI83W",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run the Greedy-GQ algorithm with linear function approximation\n",
        "def run_greedy_gq(episodes, render=False, decay=False):\n",
        "    env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
        "\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "    w = np.zeros(n_actions * n_states)  # Initialize weights for linear function approximation\n",
        "\n",
        "    alpha = 0.1  # learning rate for Q-values\n",
        "    alpha_decay_rate = 0.00001\n",
        "    beta = 0.01  # learning rate for w\n",
        "    beta_decay_rate = 0.00001\n",
        "    gamma = 0.95  # discount factor\n",
        "\n",
        "    epsilon = 1  # 1 means always the agent takes a random action\n",
        "    epsilon_decay_rate = 0.001\n",
        "\n",
        "    rng = np.random.default_rng()  # generate random number\n",
        "\n",
        "    rewards_episode = np.zeros(episodes)\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state = env.reset()[0]  # reset the environment so the agent is at state 0\n",
        "        terminated = False  # True when it reaches the goal or falls in hole\n",
        "        truncated = False  # True when actions > 200\n",
        "\n",
        "        while not terminated and not truncated:\n",
        "            action = epsilon_greedy_policy_gq(state, w, epsilon, n_actions, n_states)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            next_action = epsilon_greedy_policy_gq(next_state, w, epsilon, n_actions, n_states)\n",
        "\n",
        "            # Feature vectors for current and next state-action pairs\n",
        "            phi_s_a = get_feature_vector(state, action, n_actions, n_states)\n",
        "            phi_s_next_a = get_feature_vector(next_state, next_action, n_actions, n_states)\n",
        "\n",
        "            w = update_weights_GreedyGQ(w, state, action, reward, next_state, next_action, alpha, beta, gamma, phi_s_a, phi_s_next_a)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if render:\n",
        "                screen = env.render()\n",
        "                plt.imshow(screen)\n",
        "                ipythondisplay.clear_output(wait=True)\n",
        "                ipythondisplay.display(plt.gcf())\n",
        "                time.sleep(1)\n",
        "\n",
        "        # count a successful episode to analyze the performance of the agent\n",
        "        plt.clf()\n",
        "\n",
        "        if decay == True:\n",
        "            epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "            beta = max(beta - beta_decay_rate, 0)\n",
        "            alpha = max(alpha - alpha_decay_rate, 0)\n",
        "            if epsilon == 0:\n",
        "               alpha = 0.0001\n",
        "        if reward == 1:\n",
        "            rewards_episode[i] = 1\n",
        "\n",
        "    print(\"The number of successful episodes are:\", np.sum(rewards_episode))\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    sum_rewards = np.zeros(episodes)\n",
        "    for e in range(episodes):\n",
        "        sum_rewards[e] = np.sum(rewards_episode[max(0, e - 100):(e + 1)])\n",
        "    plt.plot(sum_rewards)\n",
        "    plt.savefig('frozen_lake_greedy_gq.png')\n",
        "\n",
        "    with open('weights_updated_greedy_gq.pkl', 'wb') as f:\n",
        "        pickle.dump(w, f)"
      ],
      "metadata": {
        "id": "DOsD6NyVfzTO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the algorithm\n",
        "run_greedy_gq(15000, render=False)"
      ],
      "metadata": {
        "id": "fUXEcTinf6i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete code for Greedy-GQ"
      ],
      "metadata": {
        "id": "D3m4Wab7hMz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run the Greedy-GQ algorithm with linear function approximation\n",
        "def run_greedy_GQ(episodes, render=False, decay=False, training=True, slippery=False):\n",
        "    env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=slippery, render_mode='rgb_array')\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    if training:\n",
        "        w = np.zeros(n_states * n_actions) # Initialize weights for linear function approximation\n",
        "    else:\n",
        "        f = open('weights_updated_greedy_gq.pkl', 'rb')\n",
        "        w = pickle.load(f)\n",
        "        f.close()\n",
        "\n",
        "    alpha = 0.1  # learning rate for Q-values\n",
        "    alpha_decay_rate = 0.00001\n",
        "    beta = 0.01  # learning rate for w\n",
        "    beta_decay_rate = 0.00001\n",
        "    gamma = 0.95  # discount factor\n",
        "\n",
        "    epsilon = 1  # 1 means always the agent takes a random action\n",
        "    epsilon_decay_rate = 0.001\n",
        "\n",
        "    rng = np.random.default_rng()  # generate random number\n",
        "\n",
        "    rewards_episode = np.zeros(episodes)\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state = env.reset()[0]  # reset the environment so the agent is at state 0\n",
        "        terminated = False  # True when it reaches the goal or falls in hole\n",
        "        truncated = False  # True when actions > 200\n",
        "\n",
        "        while not terminated and not truncated:\n",
        "\n",
        "            if training:\n",
        "              action = epsilon_greedy_policy_gq(state, w, epsilon, n_actions, n_states)\n",
        "\n",
        "              next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "              next_action = epsilon_greedy_policy_gq(next_state, w, epsilon, n_actions, n_states)\n",
        "\n",
        "            # Feature vectors for current and next state-action pairs\n",
        "              phi_s_a = get_feature_vector(state, action, n_actions, n_states)\n",
        "              phi_s_next_a = get_feature_vector(next_state, next_action, n_actions, n_states)\n",
        "\n",
        "              w = update_weights_GreedyGQ(w, state, action, reward, next_state, next_action, alpha, beta, gamma, phi_s_a, phi_s_next_a)\n",
        "\n",
        "            else:\n",
        "                q_values = np.array([np.dot(w, get_feature_vector(state, action, n_actions, n_states)) for action in range(n_actions)])\n",
        "                action = np.argmax(q_values)\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if render:\n",
        "                screen = env.render()\n",
        "                plt.imshow(screen)\n",
        "                ipythondisplay.clear_output(wait=True)\n",
        "                ipythondisplay.display(plt.gcf())\n",
        "                time.sleep(1)\n",
        "\n",
        "        # count a successful episode to analyze the performance of the agent\n",
        "        plt.clf()\n",
        "\n",
        "        if decay == True:\n",
        "            epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "            beta = max(beta - beta_decay_rate, 0)\n",
        "            alpha = max(alpha - alpha_decay_rate, 0)\n",
        "            if epsilon == 0:\n",
        "               alpha = 0.0001\n",
        "        if reward == 1:\n",
        "            rewards_episode[i] = 1\n",
        "\n",
        "    print(\"The number of successful episodes are:\", np.sum(rewards_episode))\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    sum_rewards = np.zeros(episodes)\n",
        "    for e in range(episodes):\n",
        "        sum_rewards[e] = np.sum(rewards_episode[max(0, e - 100):(e + 1)])\n",
        "    plt.plot(sum_rewards)\n",
        "    plt.savefig('frozen_lake_greedy_gq.png')\n",
        "\n",
        "    with open('weights_updated_greedy_gq.pkl', 'wb') as f:\n",
        "        pickle.dump(w, f)"
      ],
      "metadata": {
        "id": "qTBfnoL0hQNd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_greedy_GQ(15000, render=False, decay=True, training=True, slippery=False)"
      ],
      "metadata": {
        "id": "R3mVpOGhjYZN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}