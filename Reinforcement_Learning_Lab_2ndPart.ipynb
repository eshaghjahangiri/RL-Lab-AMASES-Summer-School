{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRSApGXxNh6DEuA7HaW5uY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshaghjahangiri/RL-Lab-AMASES-Summer-School/blob/main/Reinforcement_Learning_Lab_2ndPart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Portfolio Management using Reinforcement Learning Algorithms**\n",
        "\n",
        "In the previous session, we used the SARSA and Q-Learning methods to teach the agent how to navigate the **FROZEN LAKE**.\n",
        "\n",
        "**BUT, what if the state and/or action space are continuous?**\n",
        "\n",
        "---\n",
        "\n",
        "## **The Challenge**\n",
        "\n",
        "For example, in the portfolio management problem where the environment is the *Financial Market*, you need to allocate a portfolio to invest in each stock, and this amount is continuous (e.g., 0.01, 0.5, 0.42, ...). Or consider the state space of a financial market, which can include stock prices, volume of stocks traded, technical indicators, etc., all of which are not discrete!\n",
        "\n",
        "Using a Q-table to update the Q-value of each state-action pair is no longer feasible for problems where at least one of the state or action spaces is continuous.\n",
        "\n",
        "**So, what should we do?**\n",
        "\n",
        "---\n",
        "\n",
        "## **Solution: Function Approximation Methods**\n",
        "\n",
        "We turn to function approximation methods to estimate the Q-values or policies. These methods can handle continuous spaces by approximating the Q-values or policy using parameterized functions like neural networks. Some popular approaches include:\n",
        "\n",
        "### **Deep Q-Learning (DQN):**\n",
        "Extends Q-Learning by using a neural network to approximate the Q-value function. This method is particularly useful when dealing with large or continuous state spaces.\n",
        "\n",
        "### **Policy Gradient Methods:**\n",
        "Instead of learning a value function, these methods directly parameterize the policy and optimize it using gradient ascent. Examples include **REINFORCE**, **Actor-Critic**, and **Proximal Policy Optimization (PPO)**.\n",
        "\n",
        "### **Actor-Critic Methods:**\n",
        "Combine the benefits of value-based and policy-based methods. The actor decides which action to take, and the critic evaluates how good the action was, using a value function. Examples include **A2C (Advantage Actor-Critic)** and **DDPG (Deep Deterministic Policy Gradient)**.\n",
        "\n",
        "### **Continuous Action Spaces:**\n",
        "Techniques like **DDPG**, **Twin Delayed DDPG (TD3)**, and **Soft Actor-Critic (SAC)** are specifically designed to handle continuous action spaces effectively.\n",
        "\n",
        "By leveraging these advanced reinforcement learning algorithms, we can tackle complex financial environments where decisions must be made in continuous spaces.\n",
        "\n",
        "---\n",
        "\n",
        "## **Let's Implement PPO for Portfolio Management**\n",
        "\n",
        "We don't need to write advanced Deep RL algorithms from scratch!\n",
        "\n",
        "We will use the **Stable Baselines3 (SB3)** library, which provides reliable implementations of reinforcement learning algorithms.\n",
        "\n",
        "### **First Step: Define the Environment**\n",
        "\n",
        "YES! Define the environment, which is the stock market. Let's do this with the help of **Open AI Gymnasium**.\n",
        "\n",
        "---\n",
        "\n",
        "Let's get started and build an agent who can dynamically manage a financial portfolio using **PPO**!\n"
      ],
      "metadata": {
        "id": "_yzcxhg-vNUz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2N7oqNw8uUrX"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install 'shimmy>=0.2.1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n",
        "from gym import Env\n",
        "import math\n",
        "import time\n",
        "\n",
        "from stable_baselines3 import PPO, SAC, A2C"
      ],
      "metadata": {
        "id": "1m4xws7QD3s_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define an Environment\n",
        "\n",
        "### How Should an Agent Choose an Action?\n",
        "\n",
        "An action in this context is a vector of weights for the allocation of each stock. We need to determine what kind of information we want to provide to the agent. Let's imagine a portfolio manager who decides how much money to allocate to each stock based on their historical performance.\n",
        "\n",
        "### Information to Provide to the Agent:\n",
        "\n",
        "Let's give the agent the **Adjusted Close Price** of each stock at each date. Based on this information, the agent will choose how to allocate the capital among all stocks.\n",
        "\n",
        "---\n",
        "\n",
        "By providing the adjusted close prices, the agent can learn from historical data and make informed decisions on capital allocation to maximize returns. Let's proceed to set up the environment and define the necessary parameters for our agent.\n"
      ],
      "metadata": {
        "id": "pvfVUEyEHfd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_capital = 100  # initial money\n",
        "transaction_cost = 0.001\n",
        "# 10 shares per trade-share\n",
        "shares_trade = 10"
      ],
      "metadata": {
        "id": "Zb_-e19sHaUv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StockMarket(Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, df, day=0, initial=True):\n",
        "\n",
        "        super(StockMarket, self).__init__()\n",
        "\n",
        "        self.df = df\n",
        "        self.day = day\n",
        "        self.initial = initial\n",
        "\n",
        "        self.stock_num = self.df.shape[1]  # number of stocks in the portfolio\n",
        "\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(self.stock_num,))  # Action Space\n",
        "        self.observation_space = spaces.Box(low=-1, high=np.inf, shape=(2 * self.stock_num + 1,))  # State Space\n",
        "\n",
        "        self.data = self.df.iloc[self.day, :]\n",
        "\n",
        "        self.terminal = False\n",
        "\n",
        "        self.state = [initial_capital] + self.data.values.tolist() + [0] * self.stock_num  # Initial State\n",
        "\n",
        "        self.reward = 0  # Initial Reward\n",
        "\n",
        "        self.balance_memory = [initial_capital]  # Memorize the total balance\n",
        "        self.actions_memory = [[1 / self.stock_num] * self.stock_num]\n",
        "\n",
        "        self.seed(0)\n",
        "\n",
        "    def sell_stock(self, index, action):\n",
        "\n",
        "        action = np.floor(action)\n",
        "\n",
        "        if self.state[index + self.stock_num + 1] > 0:\n",
        "            self.state[0] += self.state[index + 1] * min(abs(action), self.state[index + self.stock_num + 1]) * (\n",
        "                        1 - transaction_cost)\n",
        "            self.state[index + self.stock_num + 1] -= min(abs(action), self.state[index + self.stock_num + 1])\n",
        "\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def buy_stock(self, index, action):\n",
        "\n",
        "        action = np.floor(action)\n",
        "\n",
        "        self.state[0] -= self.state[index + 1] * action * (1 + transaction_cost)\n",
        "\n",
        "        self.state[index + self.stock_num + 1] += action\n",
        "\n",
        "    def step(self, actions):\n",
        "        self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
        "\n",
        "        if self.terminal:\n",
        "            return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "        else:\n",
        "            actions = actions * shares_trade\n",
        "\n",
        "            initial_total_asset = self.state[0] + \\\n",
        "                                  sum(np.array(self.state[1:(self.stock_num + 1)]) * np.array(\n",
        "                                      self.state[(self.stock_num + 1):(self.stock_num * 2 + 1)]))\n",
        "            argsort_actions = np.argsort(actions)\n",
        "\n",
        "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
        "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
        "\n",
        "            for index in sell_index:\n",
        "                self.sell_stock(index, actions[index])\n",
        "\n",
        "            for index in buy_index:\n",
        "                self.buy_stock(index, actions[index])\n",
        "\n",
        "            self.day += 1\n",
        "            self.data = self.df.iloc[self.day, :]\n",
        "\n",
        "            self.state = [self.state[0]] + self.data.values.tolist() + \\\n",
        "                         list(self.state[(self.stock_num + 1):(self.stock_num * 2 + 1)])\n",
        "\n",
        "            final_total_asset = self.state[0] + \\\n",
        "                                sum(np.array(self.state[1:(self.stock_num + 1)]) * np.array(\n",
        "                                    self.state[(self.stock_num + 1):(self.stock_num * 2 + 1)]))\n",
        "\n",
        "            self.reward = final_total_asset - initial_total_asset\n",
        "            weights = self.normalization(np.array(self.state[(self.stock_num + 1):(self.stock_num * 2 + 1)]))\n",
        "\n",
        "            self.actions_memory.append(weights.tolist())\n",
        "            self.reward = self.reward\n",
        "\n",
        "        return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        self.day = 0\n",
        "        self.data = self.df.iloc[self.day, :]\n",
        "        self.terminal = False\n",
        "\n",
        "        self.actions_memory = [[1 / self.stock_num] * self.stock_num]  # memorize all weights changes\n",
        "\n",
        "        self.state = [initial_capital] + self.data.values.tolist() + [0] * self.stock_num\n",
        "\n",
        "        self.seed(0)\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def normalization(self, actions):\n",
        "        normalized_actions = actions / (np.sum(actions) + 1e-15)\n",
        "        return normalized_actions\n",
        "\n",
        "    def save_action_memory(self):\n",
        "        return self.actions_memory\n",
        "\n",
        "    def render(self, close=False):\n",
        "        return self.state\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "\n",
        "def train(algorithm, policy, env_traian, timesteps, seed=None, save=True):\n",
        "    start = time.time()\n",
        "    if algorithm == \"PPO\":\n",
        "        model = PPO(policy, env_traian, verbose=0, seed=seed)\n",
        "    elif algorithm == \"A2C\":\n",
        "        model = A2C(policy, env_traian, verbose=0, seed=seed)\n",
        "    elif algorithm == \"SAC\":\n",
        "        model = SAC(policy, env_traian, verbose=0, seed=seed)\n",
        "    elif algorithm == \"TD3\":\n",
        "        model = TD3(policy, env_traian, verbose=0, seed=seed)\n",
        "\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    end = time.time()\n",
        "\n",
        "    if save == True:\n",
        "        model.save(\"results/\" + algorithm + \"_\" + str(timesteps) + \"_model\")\n",
        "    print(f\"Training time: {(end - start) / 60} minutes\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_prediction(model, data, env, obs):\n",
        "    actions_memory = []\n",
        "    model.set_random_seed(10)\n",
        "    for i in range(len(data.index.unique())):\n",
        "        action, _states = model.predict(obs)\n",
        "        obs, rewards, dones, info = env.step(action)\n",
        "\n",
        "        if i == (len(data.index.unique()) - 2):\n",
        "            actions_memory = env.env_method(method_name=\"save_action_memory\")\n",
        "\n",
        "    return actions_memory[0]"
      ],
      "metadata": {
        "id": "J7pN8BxCHes2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent"
      ],
      "metadata": {
        "id": "cVMVyHDjL06C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yf\n",
        "import os"
      ],
      "metadata": {
        "id": "TiOboOzMLxM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tickers =  [\"AAPL\", \"MSFT\", \"NFLX\"]\n",
        "df = yf.download(tickers, start=\"2010-01-01\", end=\"2017-01-01\")"
      ],
      "metadata": {
        "id": "54oDu0q1L5AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df.copy()\n",
        "data['Adj Close'] = data['Adj Close'].ffill()\n",
        "df = df.ffill(axis=1)\n",
        "data['Adj Close'] = data['Adj Close'].bfill()\n",
        "df = df.bfill(axis=1)\n",
        "\n",
        "data = data['Adj Close']"
      ],
      "metadata": {
        "id": "i4-0HmN4MAiP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jKan76gdMHce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_data = []\n",
        "\n",
        "for ticker in tickers:\n",
        "    num_rows = len(data[ticker])\n",
        "    num_data.append((ticker, num_rows))\n",
        "    num_data_df = pd.DataFrame(num_data)\n",
        "\n",
        "num_data_df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yFILV_tiMRah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pct = 0.9\n",
        "train_slice = int(train_pct*len(data))\n",
        "train_data = data[:train_slice]\n",
        "test_data = data[train_slice:]"
      ],
      "metadata": {
        "id": "lQZUAcklMUOm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head(1)"
      ],
      "metadata": {
        "id": "rnQbES0NMWZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "for ticker in tickers:\n",
        "    plt.plot(data[ticker], label=ticker)\n",
        "\n",
        "plt.axvline(pd.to_datetime('2016-04-21'), color='black', linestyle='--', lw=2)\n",
        "plt.legend()\n",
        "plt.margins(x=0)\n",
        "plt.ylabel('Adj Close Price')\n",
        "plt.xlabel('Time Years')\n",
        "\n",
        "\n",
        "directory_name = 'my_images'\n",
        "folder_path = os.path.join(os.getcwd(), directory_name)\n",
        "try:\n",
        "    os.makedirs(folder_path)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "\n",
        "image_name = 'price_timeseries.png'\n",
        "image_path = os.path.join(folder_path, image_name)\n",
        "\n",
        "plt.savefig(image_path, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "Ib0KzvnIMYsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv"
      ],
      "metadata": {
        "id": "zctyqEvtMisk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rebalance_period = 1\n",
        "data_re = data.iloc[::rebalance_period, :]\n",
        "train_re = train_data.iloc[::rebalance_period, :]\n",
        "test_re = test_data.iloc[::rebalance_period, :]"
      ],
      "metadata": {
        "id": "oMxSemCpMonS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = 25000\n",
        "iterations = 3\n",
        "policy = 'MlpPolicy'\n",
        "length = test_re.shape[0]\n",
        "stocks = int(test_re.shape[1])\n"
      ],
      "metadata": {
        "id": "znaKnI10Mr8u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_re.index.min(), test_re.index.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6LQ-G5UM0a5",
        "outputId": "8287b3eb-4194-49cd-8bdd-caa017499115"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Timestamp('2016-04-21 00:00:00'), Timestamp('2016-12-30 00:00:00'))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PPO"
      ],
      "metadata": {
        "id": "weHT9vfaNCU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "algo = \"PPO\"\n",
        "returns_daily_cumulative_ppo = np.zeros([iterations, length])\n",
        "weights_portfolio_ppo = np.zeros([iterations, length, stocks])\n",
        "\n",
        "iter = 0\n",
        "cont = 0\n",
        "\n",
        "while (iter<iterations):\n",
        "    print(iter)\n",
        "    env_train = DummyVecEnv([lambda : StockMarket(df=train_re)])\n",
        "    model = train(algo, policy, env_train, timesteps, seed=cont, save=False)\n",
        "\n",
        "    env_test = DummyVecEnv([lambda : StockMarket(df=test_re)])\n",
        "    test_obs = env_test.reset()\n",
        "\n",
        "    weights_portfolio_ppo[iter] = np.array(model_prediction(model, test_re, env_test, test_obs))\n",
        "\n",
        "    check = np.sum(weights_portfolio_ppo[iter])\n",
        "    cont += 1\n",
        "    if check + 1 != length:\n",
        "        continue\n",
        "\n",
        "    return_stocks = test_re.pct_change()\n",
        "    return_stocks_ppo = np.sum(return_stocks.multiply(weights_portfolio_ppo[iter]), axis=1)\n",
        "\n",
        "    returns_daily_cumulative_ppo[iter] = (1 + return_stocks_ppo).cumprod()\n",
        "\n",
        "    iter += 1"
      ],
      "metadata": {
        "id": "mggL6pzFS6ji",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "rXtap1TlX0QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(17, 10), constrained_layout=True)\n",
        "\n",
        "ppo_mean = np.mean(np.array(returns_daily_cumulative_ppo), axis=0)\n",
        "ppo_std = np.std(np.array(returns_daily_cumulative_ppo), axis=0)\n",
        "\n",
        "axs[0, 0].plot(test_re.index, ppo_mean, color='tab:blue', linewidth=2.0)\n",
        "axs[0, 0].fill_between(test_re.index, ppo_mean - ppo_std, ppo_mean + ppo_std, alpha=0.2, color='tab:blue')\n",
        "axs[0, 0].margins(x=0)\n",
        "axs[0, 0].margins(y=0)\n",
        "axs[0, 0].axhline(1, color='black', linestyle='--', lw=2)\n",
        "axs[0, 0].set_ylabel('Cumulative Returns')\n",
        "axs[0, 0 ].set_xlabel('Time')\n",
        "\n",
        "weights_portfolio_ppo_ = np.mean(np.array(weights_portfolio_ppo), axis=0)\n",
        "df = pd.DataFrame(weights_portfolio_ppo_, index=test_re.index, columns=tickers)\n",
        "\n",
        "axs[0, 1].stackplot(test_re.index, df['AAPL'], df['MSFT'], df['NFLX'])\n",
        "axs[0, 1].legend(tickers, loc='upper right')\n",
        "axs[0, 1].margins(x=0)\n",
        "axs[0, 1].margins(y=0)\n",
        "axs[0, 1].set_ylabel(\"Weights (%)\")\n",
        "axs[0, 1].set_xlabel(\"Time\")\n",
        "\n",
        "weights_portfolio_ppo_ = weights_portfolio_ppo[np.argmax(returns_daily_cumulative_ppo[:,-1])]\n",
        "df = pd.DataFrame (weights_portfolio_ppo_, index = test_re.index, columns = tickers)\n",
        "\n",
        "axs[1, 0].stackplot(test_re.index, df['AAPL'], df['MSFT'], df['NFLX'])\n",
        "axs[1, 0].margins(x=0)\n",
        "axs[1, 0].margins(y=0)\n",
        "axs[1, 0].set_ylabel(\"Weights (%)\")\n",
        "axs[1, 0].set_xlabel(\"Time\")\n",
        "\n",
        "weights_portfolio_ppo_ = weights_portfolio_ppo[np.argmin(returns_daily_cumulative_ppo[:,-1])]\n",
        "df = pd.DataFrame (weights_portfolio_ppo_, index = test_re.index, columns = tickers)\n",
        "\n",
        "axs[1, 1].stackplot(test_re.index, df['AAPL'], df['MSFT'], df['NFLX'])\n",
        "axs[1, 1].margins(x=0)\n",
        "axs[1, 1].margins(y=0)\n",
        "axs[1, 1].set_ylabel(\"Weights (%)\")\n",
        "axs[1, 1].set_xlabel(\"Time\")"
      ],
      "metadata": {
        "id": "iIUFOAEVhV4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}